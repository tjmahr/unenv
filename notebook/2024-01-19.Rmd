<!--- Timestamp to trigger book rebuilds: `r Sys.time()` --->

```{r setup, include = FALSE}
# library(tidyverse)
# fit <- targets::tar_read(fit)
# fit
```

## January 2024

<small>Source: <code>`r knitr::current_input()`</code></small>


### One-off knitr engine for blockquoting things

Here's how easy it is to create a knitr language engine:

```{r}
# create a custom language block that blockquotes its contents
knitr::knit_engines$set("blockquote" = function(options) {
  if (isFALSE(options$echo)) return()
  paste0("> ", options$code, collapse = "\n")
})
```



Writing a chunk like:

````{verbatim}
```{blockquote}
Look at this **markdown**!
  
Here is a nested blockquote:
  
  > Cool!

Yes.
```
````

Will yield the following output:

```{blockquote}
Look at this **markdown**!
  
Here is a nested blockquote:
  
  > Cool!

Yes.
```

I wanted to make this a hook function but couldn't figure it out.

```{comment}
 This is stupid extra block to fix the syntax highlighting in RStudio.
 ````
```



### Sentence similarity

I read an
[article](https://txt.cohere.com/what-is-similarity-between-sentences/)
I had bookmarked for a while. Suppose you have word embedding (vector)
for each word. Or suppose you have a sentence embedding.

> The Cohere embedding assigns a vector of length 4096 (i.e., a list
> of 4096 numbers) to each sentence. Furthermore, the multilingual
> embedding does this for sentences in more than 100 languages. In this
> way, the sentence “Hello, how are you?” and its corresponding French
> translation, “Bonjour, comment ça va?” will be assigned very similar
> numbers, as they have the same semantic meaning.
> [[article](https://txt.cohere.com/what-is-similarity-between-sentences/)]

They have toy data about where some movies are assigned vectors:

```{r}
df_movies <- data.frame(
  row.names = c("you've got mail", "rush hour", "rush hour 2", "taken"),
  action = c(0, 6, 7, 7),
  comedy = c(5, 5, 4, 0)
) 
movies <- df_movies|> as.matrix()
movies
```

One option for similarity is the dot product of the vectors. (Why?)

```{r}
# the two rush hour movies
sum(movies[2, ] * movies[3, ])

# all at once
dot_products <- movies %*% t(movies)
```

Check this out. `as.dist()` will clean up the redundant output, but it
discards the diagonal. [Suggestion from
StackOverflow](https://stackoverflow.com/questions/2535234/find-cosine-similarity-between-two-arrays).

```{r}
as.dist(dot_products)
```

Another option is cosine similarity. They plot the points on a coordinate
space and observe:

> [...] we want a measure of similarity that is high for sentences
> that are close to each other, and low for sentences that are far away
> from each other. [Euclidean] Distance does the exact opposite. So in
> order to tweak this metric, let’s look at the angle between the rays
> from the origin (the point with coordinates [0,0]), and each
> sentence. Notice that this angle is small if the points are close to
> each other, and large if the points are far away from each other. Now
> we need the help of another function, the cosine. The cosine of angles
> close to zero is close to 1 [...]
> [[article](https://txt.cohere.com/what-is-similarity-between-sentences/)] 

According to
[Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity), we can
derive the cosine similarity from the dot-product:

$$
\begin{aligned} 
\mathbf{A}\cdot\mathbf{B}&=\left\|\mathbf{A}\right\|\left\|\mathbf{B}\right\|\cos\theta \\ 
{\mathbf{A} \cdot \mathbf{B} \over \|\mathbf{A}\| \|\mathbf{B}\|} &= \cos(\theta) \\
\|\mathbf{m}\| &:= \sqrt{\mathbf{m} \cdot \mathbf{m}} ~~~~~\text{(Euclidean norm)}
\end{aligned}
$$

We have a matrix with all of the dot products ready to go, so we need a
matrix of all norms multiplied together.

```{r}
dot_products

norms <- sqrt(rowSums(movies ^ 2))
norm_products <- norms %*% t(norms)

(dot_products / norm_products) |> as.dist() |> round(3)
```

This value is cosine similarity. If we use 1 - similarity, it's cosine
distance. That would be more appropriate for anything piped into
`as.dist()`.

Still, two points could be very different distances from the origin, but
they could have an angle of 0.

### How I got targets to handle a bunch of bootstrapping

Here is a summary:

  - transient memory
  - not letting errors on one branch stop the whole pipeline
  - `tarchetypes::tar_group_by()`
  - `format = "fst_tbl"` or `format = "qs"`
  - `tar_read(..., 1)`

My bootstraps would instantly fill up my computer's memory during a
build. So now I dump stuff out of memory with `"transient"`. (You can do
this on a by-target level too.) I also keep running the build if one
target/branch fails:

```{r, eval = FALSE}
tar_option_set(
  error = "null",
  memory = "transient"
)
```

I also split a single target into smaller ones. I declare a global
partition value like

```{r}
N_PARTITIONS <- 80
```

Then I used `tarchetypes::tar_group_by()` to tell targets to split a
giant dataframe into groups that can be dynamically branched over. Here
is some actual code in the `_targets.R` file. The first targets
defines 2000 bootstrap replicates to be split into 80
partitions/branches and the second fits the models on each
partition/branch.

```{r, eval = FALSE}
  tar_group_by(
    straps,
    data_iwpm |>
      filter(!is.na(mean_wpm)) |>
      prepare_bootstrap_data() |>
      bootstrap_by_slpg(
        times = n_boot,
        col_child_id = subject_id,
        seed = 20220621
      ) |>
      mutate(partition = assign_partition_numbers(id, N_PARTITIONS)),
    partition
  ),
  tar_target(
    models_intelligibility,
    fit_bootstrap_models(straps, "mean_intel", "intelligibility", 3, 2),
    pattern = map(straps),
    format = "qs"
  ),
# btw the targets here themselves are built serially bc fit_bootstrap_models()
# and friends use furrr to fit the models in each branch in parallel
```

But to be honest, as I'm looking at this code, I'm realizing that this
"making a giant thing and split into branches" have been built as "make
a small table of branch IDs and grow them within each branch". Analogy
would be to read in a dozen files at once and split up the work so each
file is processed separately versus take a dozen filenames and handle
from each filename separately.

I have to use `tarchetypes::tar_group_by()` instead of
`tarchetypes::tar_group_count()` because I have multiple datasets that I
am bootstrapping and they need to be kept together within a branch.
Like, to split the following into two branches, I would want
`bootstrap_number` 1 and 2 in one branch and 3 in a separate branch so
that I can compare `a`, `b` and `c` within each bootstrap.

```
data_set,bootstrap_number
a,1
b,1
c,1
a,2
b,2
c,2
a,3
b,3
c,3
```

For serializing the data, I used `format = "qs"` on models or lists of
models and `format = "fst_tbl"` on plain, inoffensive data tables.

I also started using `targets::tar_read(blah, n)` where `n` is some
selection of branches like `1` or `c(1, 2, 4)`, to read a subset of the
overall data. This speeds up interactive development by giving me a
smaller subset of data to write code around without having to load
gigantic objects into R. 


### targets + crew

In targets, by default, targets are dispatched to be built one at a
time, but we can use crew so that multiple workers can build targets in
parallel. Here is the code to add to `_targets.R` to get 2 workers.

```{r eval = FALSE}
targets::tar_option_set(
  controller = crew::crew_controller_local(workers = 2)
)
```

And then you can view how much work each worker did. (I'm hard-coding
the returns to show what happened with a full sit build.)

```{r, eval = FALSE}
targets::tar_crew()
#>   controller               worker seconds targets
#>   <chr>                     <int>   <dbl>   <int>
#> 1 8f28e966dd6adaf60895eb01      1    49.4      32
#> 2 8f28e966dd6adaf60895eb01      2    25.1       9
```

### MFA note: don't touch the solver

My colleague and I independently ran into an issue where the Montreal
Forced Aligner installation with conda seemed to take forever during the
solving environment stage. We both tried to fix this problem by changing
the default solver. This change did lead to improved installation, but
the MFA could not run at all. The correct thing to do is **do nothing**.
Just use the default solver and wait it out.

### Revelation while reading withr 3.0.0 release

I was reading the [withr 3.0.0 release
notes](https://www.tidyverse.org/blog/2024/01/withr-3-0-0/) and noticed
a pleasing symmetry the `with_` and `local_` functions.

The `local_` functions set a temporary value within some local scope,
like a `local()` or a `function()`.

```{r}
local({
  withr::local_language("fr")
  plot[[1]]
})
plot[[1]]
```

The `with_` functions set a temporary value around some block of code.

```{r}
withr::with_language("fr", {
  plot[[1]]
})
plot[[1]]
```

So where are we working from?

- `local_`: within the code (scope)
- `with_`: around the code

Oh, and the notes for withr 3.0.0, indicates that I should keep an eye
on how `source()` interacts with withr.

### A silly function

I got sick of writing null-guard clauses like:

```{r}
my_cool_function <- function(data, ...) {
  if (is.null(data)) { 
    return(NULL)
  }
  
  #  rest of the function
  nrow(data)
}
```

So why can't I just make that a higher order function:

```{r}
nullsafe_map <- function(x, f, ...) {
  if (is.null(x)) NULL else f(x, ...)
}

mtcars |> 
  nullsafe_map(my_cool_function) |> 
  nullsafe_map(log)

NULL |> 
  nullsafe_map(my_cool_function) |> 
  nullsafe_map(log)
```

It feels like I reinvented something vaguely monad-y here, and [I posted
the
question](https://staging.bsky.app/profile/tjmahr.com/post/3kiq3hhlvis2m):
"What have I reinvented here?" Tan suggested "`purrr::map_if()` with `.p
= Negate(is.null)`?" which is cool because I had forgotten about
`purrr::map_if()`.

### The pipe placeholder works better now

[Andrew Heiss pointed out to
me](https://staging.bsky.app/profile/andrew.heiss.phd/post/3ki4b46li6l2y)
that the placeholder since 4.3.0 is a lot better. 

```{r}
quote(
  model |> _$coef |> _[1]
) 
```

Previously, I had been circumventing these functions with things like
`getElement()` or making my own pipe-friendly wrapper functions like:

```{r}
vec_element <- function(xs, i) {
  xs[[i]]
}

vec_index <- function(xs, i) {
  xs[i]
}

vec_index_into <- function(i, xs) {
  xs[i]
}

vec_replace_na <- function(xs, replacement) {
  xs[is.na(xs)] <- replacement
  xs
}

vec_remove_na <- function(xs) {
  xs[!is.na(xs)]
}

vec_set_names <- function(xs, ns) {
  names(xs) <- ns
  xs
}
```

There were two other functions alongside these helpers that weren't
meant to solve any specific piping problem. I came to adore them when
I was doing some Advent of Code puzzles for fun in base R.

```{r}
vec_which_value <- function(xs, value, negate = FALSE) {
  (xs %in% value) |>
    xor(negate) |>
    which()
}

vec_which_name <- function(xs, name, negate = FALSE) {
  vec_which_value(names(xs), name, negate)
}

letters |> 
  vec_which_value(c("a", "e", "i", "o", "u"), negate = TRUE) |> 
  vec_index_into(letters)
```

### `purrr::pluck()` is better than I had realized

From `?purrr::pluck()`:

> [`pluck()`] also accepts arbitrary accessor functions, i.e.
> functions that take an object and return some internal piece. [...]
> Compare: `accessor(x[[1]])$foo` to `pluck(x, 1, accessor, "foo")`.


### Mutual recursion demo

I toyed with the experimental `Tailcall` feature in R-devel. I can't
actually run this code when rendering the notebook, but the basic idea
is that `sys.nframe()` shows the height of the stack when the recursion
bottoms out and it is much smaller in tha `Tailcall` examples.

```{r, eval = FALSE}
is_even <- function(n) {
  if (n == 0) list(TRUE, sys.nframe()) else Tailcall(is_odd, n - 1)
}
is_odd <- function(n) {
  if (n == 0) list(FALSE, sys.nframe()) else Tailcall(is_even, n - 1)
}
is_odd(30)

naive_is_even <- function(n) {
  if (n == 0) list(TRUE, sys.nframe()) else naive_is_odd(n - 1)
}
naive_is_odd <- function(n) {
  if (n == 0) list(FALSE, sys.nframe()) else naive_is_even(n - 1)
}
naive_is_odd(30)
```


### Speaking of tail calls, how to write an "iterative" recursive function

I revisited *SICP* during the Christmas break. I don't know why this stuck
out to me this time---it's a chapter 1 topic, I have definitely seen
it---but SICP differentiates between two kinds of recursive functions.
Here is the example in R with factorial. First, let's make sure that
multiplication grows the stack by making it a function call instead of 
whatever kind of primitive `*` is.

```{r}
`%times%` <- function(a, b) a * b

fact1 <- function(n) {
  if (n == 0) {
    message("nframe of fact1: ", sys.nframe())
    1
  } else {
    n %times% fact1(n - 1)
  }
}

fact2 <- function(n) {
  fact_iter <- function(n, accumulator) {
    if (n == 0) {
      message("nframe of fact2: ", sys.nframe())
      1 %times% accumulator
    } else {
      accumulator <- n %times% accumulator
      n <- n - 1
      fact_iter(n, accumulator)
    }
  }
  fact_iter(n, 1)
}

message("nframe of this code block: ", sys.nframe())
fact1(10)
fact2(10)
```

The difference is that `n %times% fact(n - 1)` in `fact()` grows into a
giant single multiplication but `fact_iter(n - 1, n %times%
accumulator)` in `fact2()` does not.

The *SICP* authors [describe the difference](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e1) between the two like so:

> Consider the first process. [...] The expansion occurs as the
> process builds up a chain of *deferred operations* (in this case, a
> chain of multiplications). [...] This type of process, characterized
> by a chain of deferred operations, is called a *recursive process*.
> Carrying out this process requires that the interpreter keep track of
> the operations to be performed later on. In the computation of $n!$,
> the length of the chain of deferred multiplications, and hence the
> amount of information needed to keep track of it, grows linearly with
> $n$ (is proportional to $n$), just like the number of steps. [...]
> 
> [In the second process, at] each step, all we need to keep track of,
> for any $n$, are the current values of the variables [`n` and
> `accumulator` in my code]. We call this *an iterative process*. In
> general, an iterative process is one whose state can be summarized by
> a fixed number of *state variables*, together with a fixed rule that
> describes how the state variables should be updated as the process
> moves from state to state and an (optional) end test that specifies
> conditions under which the process should terminate.

With something like Fibonacci, where two recursively defined things are
added together, means that the number of deferred additions grows
exponentially.
