<!--- Timestamp to trigger book rebuilds: `r Sys.time()` --->

```{r setup, include = FALSE}
library(tidyverse)
```


## January 2025

<small>Source: <code>`r knitr::current_input()`</code></small>

Another year, another shot at taking notes.

### Use GAMs for assessing nonlinearity of effects

A [Data Colada post](https://datacolada.org/121) describes a problem and
one proposed solution: how to assess a moderator's interaction in a
model like `y ~ x + z + x:z`. One paper suggests binning the covariate
into thirds and fitting separate regressions on the bins. Uri Simonsohn
observes that this approach is biased when the `y ~ x` or `y ~ z`
relationship is nonlinear.

> The third problem is that if x and z in that xÂ·z interaction are
> correlated, and either x or z impacts y non-linearly, the estimate of
> interaction term, d, in y=a+bx+cz+dxz is biased, and the binning
> estimator from the Political Analysis paper is also biased, possibly
> by the same amount.
> 
> Most notably, one is likely to find false-positive interactions, and
> marginal effects of the wrong sign.

The author instead advocates for using a "GAM simple slope" and point to
[their article
(DOI: 10.1177/2515245923120778)](https://urisohn.com/sohn_files/papers/interacting.pdf). 

The thing that I like most about the article from skimming it is that it
describes curves that are linear and ceiling out as "canopy" shaped.

### `sweep()` demo

`sweep(data, margin, stats, fun)` is a goofy function for adjusting cols
or rows. For example, you can subtract the mean and divide by the SD to
get *z*-scores:

```{r}
d <- mtcars
d_scale <- d |> lapply(scale) |> as.data.frame()
d_sweep <- d |> 
  sweep(2, colMeans(d), `-`) |> 
  sweep(2, apply(d, 2, sd), `/`)
all(d_scale$mpg == d_sweep$mpg)
```

### Log-likelihood functions

These two ways of computing the log-likelihood (an automatic way and
naive way) differ because `sigma(m)` estimates sigma so it applies a (1
/ (N - p)) correction but `logLik()` does not.

```{r}
m <- lm(mpg ~ wt + disp, mtcars)
logLik(m)

ls <- dnorm(mtcars$mpg, predict(m), sigma(m))
sum(log(ls))
```


### Bits and pieces

ðŸ”— a [good
distribution](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14387)
for count data (DOI: 10.1111/2041-210X.14387)

ðŸ“Œ use `parallelly::availableCores()` instead of
`parallel::detectCores()` because of [problems with the
latter](https://www.jottr.org/2022/12/05/avoid-detectcores/)

ðŸ”— [flint R package](https://github.com/etiennebacher/flint) for fast
code linting and fixing lints. I like how the author tested the package
by linting and repairing huge R packages (e.g., ggplot2 or targets) and
getting the patches accepted.


**Default for an NA value**. A trick in the linked post on
`parallel::detectCores()` to replace a possible `NA` with a safe
default: `max(default, possible_na, na.rm = TRUE)`.

**Random variables**. ["A random variable is not random and not a
variable"](https://youtu.be/KQHfOZHNZ3k). Yep, it's a random number
generator or a function. Lol, just for fun, let's make a random variable
into an actual random variable (a code variable that emits a random
value every time it is evaluated):

```{r}
makeActiveBinding("die_value", function(x) sample(1:6, 1), .GlobalEnv)
die_value
c(die_value, die_value, die_value, die_value)


mean(replicate(1000, die_value))
mean(1:6)
```

