<!--- Timestamp to trigger book rebuilds: `r Sys.time()` --->

```{r setup, include = FALSE}
library(tidyverse)
```




## June 2025

<small>Source: <code>`r knitr::current_input()`</code></small>

### Mundlak machines

I've been going down a rabbithole about centering in multilevel models and all
that fun stuff. Important resources so far:

- Hazlett, C., & Wainstein, L. (2022). Understanding, Choosing, and Unifying Multilevel and Fixed Effect Approaches. Political Analysis, 30(1), 46–65. https://doi.org/10.1017/pan.2020.41

- Hoffman, L., & Walters, R. W. (2022). Catching Up on Multilevel Modeling. Annual Review of Psychology, 73(1), 659–689. https://doi.org/10.1146/annurev-psych-020821-103525

- Guo, Y., Dhaliwal, J., & Rights, J. D. (2024). Disaggregating level-specific effects in cross-classified multilevel models. Behavior Research Methods, 56(4), 3023–3057. https://doi.org/10.3758/s13428-023-02238-7

- McElreath, R. (2023). Statistical Rethinking 2023 - 12 - Multilevel Models. [*Relevant portion at 54:47.*] https://www.youtube.com/watch?v=iwVqiiXYeC4&t=3286s

- Bell, A., Jones, K., & Fairbrother, M. (2018). Understanding and misunderstanding group mean centering: A commentary on Kelley et al.’s dangerous practice. Quality & Quantity, 52(5), 2031–2036. https://doi.org/10.1007/s11135-017-0593-5

The Mundlak problem happens when we have a repeated measure outcome *y*, a
repeated measure predictor *x* both nested in groups *g*. The naive
multilevel model to estimate would be `y ~ 1 + x + (x | g)`. If there is
no repeated predictor *x*, we don't have to worry about this particular
problem. 


```{r}

```





*The practical modeling rationale*. We can frame the problem in terms of "smushing" or "conflation" of
between-group and within-group effects. Suppose that we don't use a
multilevel model and just average at the group level and model `y_mean ~
x_mean`. This aggregated model can assess whether groups with higher
average x values have higher expected average y values (a between-group
effect). But the more interesting model is the disaggregated one where
we examine---at the observation level or row level---whether changes in
*x* leads to changes in *y* in each group (a within-group effect). When
we just include `x` in the model, we are estimating both between and
within group effects behind a single coefficient. So, with a pragmatic
interest in disentangling these effects, we should separate these
effects by including the group means as a predictor: `y ~ 1 + x + x_mean
+ (x | g)`.

Bell, Jones and Fairbrother (2018) provide a version of this rationale:

> Kelley et al. argue in favour of the standard RE model (model 1), over the
model with group-mean-centering, model 2. The limitation of their preferred
model, however, is that the estimated coefficient on x<sub>ij</sub> is a
weighted average of two effects—one at level 1 and the other at level 2 (Bell et
al. 2016; Bell and Jones 2015; Fairbrother 2014). These two effects have
different substantive meanings; they capture the within-group and between-group
relationships, respectively, and these relationships may be quite different. At
level 1, the ‘within’ effect captures the difference on Y between units that are
higher or lower *than average on X relative to their group**, whilst at level 2
the ‘between’ or ‘contextual’ effect captures the difference between *groups
that have a higher or lower X as a whole (or, equivalently, on average)*.

Some groups can have higher average `x` values than other groups, so we might want to know 

there is a between-group variability. The value of `x` can vary within group, and the effect of `x` on `y` can diffe







## January 2025

<small>Source: <code>`r knitr::current_input()`</code></small>

Another year, another shot at taking notes.

### Use GAMs for assessing nonlinearity of effects

A [Data Colada post](https://datacolada.org/121) describes a problem and
one proposed solution: how to assess a moderator's interaction in a
model like `y ~ x + z + x:z`. One paper suggests binning the covariate
into thirds and fitting separate regressions on the bins. Uri Simonsohn
observes that this approach is biased when the `y ~ x` or `y ~ z`
relationship is nonlinear.

> The third problem is that if x and z in that x·z interaction are
> correlated, and either x or z impacts y non-linearly, the estimate of
> interaction term, d, in y=a+bx+cz+dxz is biased, and the binning
> estimator from the Political Analysis paper is also biased, possibly
> by the same amount.
> 
> Most notably, one is likely to find false-positive interactions, and
> marginal effects of the wrong sign.

The author instead advocates for using a "GAM simple slope" and point to
[their article
(DOI: 10.1177/2515245923120778)](https://urisohn.com/sohn_files/papers/interacting.pdf). 

The thing that I like most about the article from skimming it is that it
describes curves that are linear and ceiling out as "canopy" shaped.

### `sweep()` demo

`sweep(data, margin, stats, fun)` is a goofy function for adjusting cols
or rows. For example, you can subtract the mean and divide by the SD to
get *z*-scores:

```{r}
d <- mtcars
d_scale <- d |> lapply(scale) |> as.data.frame()
d_sweep <- d |> 
  sweep(2, colMeans(d), `-`) |> 
  sweep(2, apply(d, 2, sd), `/`)
all(d_scale$mpg == d_sweep$mpg)
```

### Log-likelihood functions

These two ways of computing the log-likelihood (an automatic way and
naive way) differ because `sigma(m)` estimates sigma so it applies a (1
/ (N - p)) correction but `logLik()` does not.

```{r}
m <- lm(mpg ~ wt + disp, mtcars)
logLik(m)

ls <- dnorm(mtcars$mpg, predict(m), sigma(m))
sum(log(ls))
```


### Bits and pieces

  - a [good
    distribution](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14387)
    for count data (DOI: 10.1111/2041-210X.14387)
  - use `parallelly::availableCores()` instead of
    `parallel::detectCores()` because of [problems with the
    latter](https://www.jottr.org/2022/12/05/avoid-detectcores/)
    - [flint R package](https://github.com/etiennebacher/flint) for fast
    code linting and fixing lints. I like how the author tested the
    package by linting and repairing huge R packages (e.g., ggplot2 or
    targets) and getting the patches accepted.


**Default for an NA value**. A trick in the linked post on
`parallel::detectCores()` to replace a possible `NA` with a safe
default: `max(default, possible_na, na.rm = TRUE)`.

**Random variables**. ["A random variable is not random and not a
variable"](https://youtu.be/KQHfOZHNZ3k). Yep, it's a random number
generator or a function. Lol, just for fun, let's make a random variable
into an actual random variable (a code variable that emits a random
value every time it is evaluated):

```{r}
makeActiveBinding("die_value", function(x) sample(1:6, 1), .GlobalEnv)
die_value
c(die_value, die_value, die_value, die_value)


mean(replicate(1000, die_value))
mean(1:6)
```

