<!--- Timestamp to trigger book rebuilds: 2024-01-19 15:25:55.988002 --->



## January 2024

<small>Source: <code>2024-01-19.Rmd</code></small>

### How I got targets to handle a bunch of bootstrapping

Here is a summary:

  - transient memory
  - not letting errors on one branch stop the whole pipeline
  - `tarchetypes::tar_group_by()`
  - `format = "fst_tbl"` or `format = "qs"`
  - `tar_read(..., 1)`

My bootstraps would instantly fill up my computer's memory during a
build. So now I dump stuff out of memory with `"transient"`. (You can do
this on a by-target level too.) I also keep running the build if one
target/branch fails:


```r
tar_option_set(
  error = "null",
  memory = "transient"
)
```

I also split a single target into smaller ones. I declare a global
partition value like


```r
N_PARTITIONS <- 80
```

Then I used `tarchetypes::tar_group_by()` to tell targets to split a
giant dataframe into groups that can be dynamically branched over. Here
is some actual code in the `_targets.R` file. The first targets
defines 2000 bootstrap replicates to be split into 80
partitions/branches and the second fits the models on each
partition/branch.


```r
  tar_group_by(
    straps,
    data_iwpm |>
      filter(!is.na(mean_wpm)) |>
      prepare_bootstrap_data() |>
      bootstrap_by_slpg(
        times = n_boot,
        col_child_id = subject_id,
        seed = 20220621
      ) |>
      mutate(partition = assign_partition_numbers(id, N_PARTITIONS)),
    partition
  ),
  tar_target(
    models_intelligibility,
    fit_bootstrap_models(straps, "mean_intel", "intelligibility", 3, 2),
    pattern = map(straps),
    format = "qs"
  ),
# btw the targets here themselves are built serially bc fit_bootstrap_models()
# and friends use furrr to fit the models in each branch in parallel
```

But to be honest, as I'm looking at this code, I'm realizing that this
"making a giant thing and split into branches" have been built as "make
a small table of branch IDs and grow them within each branch". Analogy
would be to read in a dozen files at once and split up the work so each
file is processed separately versus take a dozen filenames and handle
from each filename separately.

I have to use `tarchetypes::tar_group_by()` instead of
`tarchetypes::tar_group_count()` because I have multiple datasets that I
am bootstrapping and they need to be kept together within a branch.
Like, to split the following into two branches, I would want
`bootstrap_number` 1 and 2 in one branch and 3 in a separate branch so
that I can compare `a`, `b` and `c` within each bootstrap.

```
data_set,bootstrap_number
a,1
b,1
c,1
a,2
b,2
c,2
a,3
b,3
c,3
```

For serializing the data, I used `format = "qs"` on models or lists of
models and `format = "fst_tbl"` on plain, inoffensive data tables.

I also started using `targets::tar_read(blah, n)` where `n` is some
selection of branches like `1` or `c(1, 2, 4)`, to read a subset of the
overall data. This speeds up interactive development by giving me a
smaller subset of data to write code around without having to load
gigantic objects into R. 


### MFA note: don't touch the solver

My colleague and I independently ran into an issue where the Montreal
Forced Aligner installation with conda seemed to take forever during the
solving environment stage. We both tried to fix this problem by changing
the default solver. This change did lead to improved installation, but
the MFA could not run at all. The correct thing to do is **do nothing**.
Just use the default solver and wait it out.

### targets + crew

In targets, by default, targets are dispatched to be built one at a
time, but we can use crew so that multiple workers can build targets in
parallel. Here is the code to add to `_targets.R` to get 2 workers.


```r
targets::tar_option_set(
  controller = crew::crew_controller_local(workers = 2)
)
```

And then you can view how much work each worker did. (I'm hard-coding
the returns to show what happened with a full sit build.)


```r
targets::tar_crew()
#>   controller               worker seconds targets
#>   <chr>                     <int>   <dbl>   <int>
#> 1 8f28e966dd6adaf60895eb01      1    49.4      32
#> 2 8f28e966dd6adaf60895eb01      2    25.1       9
```

### Revelation while reading withr 3.0.0 release

I was reading the [withr 3.0.0 release
notes](https://www.tidyverse.org/blog/2024/01/withr-3-0-0/) and noticed
a pleasing symmetry the `with_` and `local_` functions.

The `local_` functions set a temporary value within some local scope,
like a `local()` or a `function()`.


```r
local({
  withr::local_language("fr")
  plot[[1]]
})
#> Error in plot[[1]]: objet de type 'closure' non indiçable
plot[[1]]
#> Error in plot[[1]]: object of type 'closure' is not subsettable
```

The `with_` functions set a temporary value around some block of code.


```r
withr::with_language("fr", {
  plot[[1]]
})
#> Error in plot[[1]]: objet de type 'closure' non indiçable
plot[[1]]
#> Error in plot[[1]]: object of type 'closure' is not subsettable
```

So where are we working from?

- `local_`: within the code (scope)
- `with_`: around the code

Oh, and the notes for withr 3.0.0, indicates that I should keep an eye
on how `source()` interacts with withr.

### A silly function

I got sick of writing null-guard clauses like:


```r
my_cool_function <- function(data, ...) {
  if (is.null(data)) { 
    return(NULL)
  }
  
  #  rest of the function
  nrow(data)
}
```

So why can't I just make that a higher order function:


```r
nullsafe_map <- function(x, f, ...) {
  if (is.null(x)) NULL else f(x, ...)
}

mtcars |> 
  nullsafe_map(my_cool_function) |> 
  nullsafe_map(log)
#> [1] 3.465736

NULL |> 
  nullsafe_map(my_cool_function) |> 
  nullsafe_map(log)
#> NULL
```

It feels like I reinvented something vaguely monad-y here, and [I posted
the
question](https://staging.bsky.app/profile/tjmahr.com/post/3kiq3hhlvis2m):
"What have I reinvented here?" Tan suggested "`purrr::map_if()` with `.p
= Negate(is.null)`?" which is cool because I had forgotten about
`purrr::map_if()`.

### The pipe placeholder works better now

[Andrew Heiss pointed out to
me](https://staging.bsky.app/profile/andrew.heiss.phd/post/3ki4b46li6l2y)
that the placeholder since 4.3.0 is a lot better. 


```r
quote(
  model |> _$coef |> _[1]
) 
#> model$coef[1]
```

Previously, I had been circumventing these functions with things like
`getElement()` or making my own pipe-friendly wrapper functions like:


```r
vec_element <- function(xs, i) {
  xs[[i]]
}

vec_index <- function(xs, i) {
  xs[i]
}

vec_index_into <- function(i, xs) {
  xs[i]
}

vec_replace_na <- function(xs, replacement) {
  xs[is.na(xs)] <- replacement
  xs
}

vec_remove_na <- function(xs) {
  xs[!is.na(xs)]
}

vec_set_names <- function(xs, ns) {
  names(xs) <- ns
  xs
}
```

There were two other functions alongside these helpers that weren't
meant to solve any specific piping problem. I came to adore them when
I was doing some Advent of Code puzzles for fun in base R.


```r
vec_which_value <- function(xs, value, negate = FALSE) {
  (xs %in% value) |>
    xor(negate) |>
    which()
}

vec_which_name <- function(xs, name, negate = FALSE) {
  vec_which_value(names(xs), name, negate)
}

letters |> 
  vec_which_value(c("a", "e", "i", "o", "u"), negate = TRUE) |> 
  vec_index_into(letters)
#>  [1] "b" "c" "d" "f" "g" "h" "j" "k" "l" "m" "n" "p" "q" "r" "s" "t" "v" "w" "x"
#> [20] "y" "z"
```

### `purrr::pluck()` is better than I had realized

From `?purrr::pluck()`:

> [`pluck()`] also accepts arbitrary accessor functions, i.e.
> functions that take an object and return some internal piece. [...]
> Compare: `accessor(x[[1]])$foo` to `pluck(x, 1, accessor, "foo")`.


### Mutual recursion demo

I toyed with the experimental `Tailcall` feature in R-devel. I can't
actually run this code when rendering the notebook, but the basic idea
is that `sys.nframe()` shows the height of the stack when the recursion
bottoms out and it is much smaller in tha `Tailcall` examples.


```r
is_even <- function(n) {
  if (n == 0) list(TRUE, sys.nframe()) else Tailcall(is_odd, n - 1)
}
is_odd <- function(n) {
  if (n == 0) list(FALSE, sys.nframe()) else Tailcall(is_even, n - 1)
}
is_odd(30)

naive_is_even <- function(n) {
  if (n == 0) list(TRUE, sys.nframe()) else naive_is_odd(n - 1)
}
naive_is_odd <- function(n) {
  if (n == 0) list(FALSE, sys.nframe()) else naive_is_even(n - 1)
}
naive_is_odd(30)
```


### Speaking of tail calls, how to write an "iterative" recursive function

I revisited *SICP* during the Christmas break. I don't know why this stuck
out to me this time---it's a chapter 1 topic, I have definitely seen
it---but SICP differentiates between two kinds of recursive functions.
Here is the example in R with factorial. First, let's make sure that
multiplication grows the stack by making it a function call instead of 
whatever kind of primitive `*` is.


```r
`%times%` <- function(a, b) a * b

fact1 <- function(n) {
  if (n == 0) {
    message("nframe of fact1: ", sys.nframe())
    1
  } else {
    n %times% fact1(n - 1)
  }
}

fact2 <- function(n) {
  fact_iter <- function(n, accumulator) {
    if (n == 0) {
      message("nframe of fact2: ", sys.nframe())
      1 %times% accumulator
    } else {
      accumulator <- n %times% accumulator
      n <- n - 1
      fact_iter(n, accumulator)
    }
  }
  fact_iter(n, 1)
}

message("nframe of this code block: ", sys.nframe())
#> nframe of this code block: 39
fact1(10)
#> nframe of fact1: 60
#> [1] 3628800
fact2(10)
#> nframe of fact2: 51
#> [1] 3628800
```

The difference is that `n %times% fact(n - 1)` in `fact()` grows into a
giant single multiplication but `fact_iter(n - 1, n %times%
accumulator)` in `fact2()` does not.

The *SICP* authors [describe the difference](https://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e1) between the two like so:

> Consider the first process. [...] The expansion occurs as the
> process builds up a chain of *deferred operations* (in this case, a
> chain of multiplications). [...] This type of process, characterized
> by a chain of deferred operations, is called a *recursive process*.
> Carrying out this process requires that the interpreter keep track of
> the operations to be performed later on. In the computation of $n!$,
> the length of the chain of deferred multiplications, and hence the
> amount of information needed to keep track of it, grows linearly with
> $n$ (is proportional to $n$), just like the number of steps. [...]
> 
> [In the second process, at] each step, all we need to keep track of,
> for any $n$, are the current values of the variables [`n` and
> `accumulator` in my code]. We call this *an iterative process*. In
> general, an iterative process is one whose state can be summarized by
> a fixed number of *state variables*, together with a fixed rule that
> describes how the state variables should be updated as the process
> moves from state to state and an (optional) end test that specifies
> conditions under which the process should terminate.

With something like Fibonacci, where two recursively defined things are
added together, means that the number of deferred additions grows
exponentially.
